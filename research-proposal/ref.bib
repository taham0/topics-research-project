@article{DBLP:journals/corr/McMahanMRA16,
  author    = {H. Brendan McMahan and
               Eider Moore and
               Daniel Ramage and
               Blaise Ag{\"{u}}era y Arcas},
  title     = {Federated Learning of Deep Networks using Model Averaging},
  journal   = {CoRR},
  volume    = {abs/1602.05629},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.05629},
  eprinttype = {arXiv},
  eprint    = {1602.05629},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/McMahanMRA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1812-01097,
  author    = {Sebastian Caldas and
               Peter Wu and
               Tian Li and
               Jakub Kone{\v{c}}n{\'y} and
               H. Brendan McMahan and
               Virginia Smith and
               Ameet Talwalkar},
  title     = {{LEAF:} {A} Benchmark for Federated Settings},
  journal   = {CoRR},
  volume    = {abs/1812.01097},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.01097},
  eprinttype = {arXiv},
  eprint    = {1812.01097},
  timestamp = {Wed, 23 Dec 2020 09:35:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-01097.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3446382.3448652,
author = {Naseer, Usama and Benson, Theophilus A. and Netravali, Ravi},
title = {WebMedic: Disentangling the Memory-Functionality Tension for the Next Billion Mobile Web Users},
year = {2021},
isbn = {9781450383233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446382.3448652},
doi = {10.1145/3446382.3448652},
abstract = {Users in developing regions still suffer from poor web performance, mainly due to their unique landscape of low-end devices. In this paper, we uncover a root cause of this suboptimal performance by cross-analyzing longitudinal resource (in particular, memory) profiles from a large social network, and the memory consumption of modern webpages in five regions. We discover that the primary culprit for hitting memory constraints is JavaScript execution which existing optimizations are ill-suited to alleviate. To handle this, we propose WebMedic, an approach that trades-off less critical functionality of a webpage to directly address memory and performance problems.},
booktitle = {Proceedings of the 22nd International Workshop on Mobile Computing Systems and Applications},
pages = {71–77},
numpages = {7},
keywords = {Mobile web, Memory performance, Web optimizations},
location = {Virtual, United Kingdom},
series = {HotMobile '21}
}

@INPROCEEDINGS{9225395,
  author={Vamsi, G Krishna and Rasool, Akhtar and Hajela, Gaurav},
  booktitle={2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)}, 
  title={Chatbot: A Deep Neural Network Based Human to Machine Conversation Model}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/ICCCNT49239.2020.9225395}}

@article{DBLP:journals/corr/abs-1812-06127,
  author    = {Anit Kumar Sahu and
               Tian Li and
               Maziar Sanjabi and
               Manzil Zaheer and
               Ameet Talwalkar and
               Virginia Smith},
  title     = {On the Convergence of Federated Optimization in Heterogeneous Networks},
  journal   = {CoRR},
  volume    = {abs/1812.06127},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.06127},
  eprinttype = {arXiv},
  eprint    = {1812.06127},
  timestamp = {Wed, 23 Dec 2020 09:35:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-06127.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2110-14205,
  author    = {Muhammad Tahir Munir and
               Muhammad Mustansar Saeed and
               Mahad Ali and
               Zafar Ayyub Qazi and
               Ihsan Ayyub Qazi},
  title     = {FedPrune: Towards Inclusive Federated Learning},
  journal   = {CoRR},
  volume    = {abs/2110.14205},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.14205},
  eprinttype = {arXiv},
  eprint    = {2110.14205},
  timestamp = {Fri, 29 Oct 2021 10:49:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-14205.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2010-01264,
  author    = {Enmao Diao and
               Jie Ding and
               Vahid Tarokh},
  title     = {HeteroFL: Computation and Communication Efficient Federated Learning
               for Heterogeneous Clients},
  journal   = {CoRR},
  volume    = {abs/2010.01264},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.01264},
  eprinttype = {arXiv},
  eprint    = {2010.01264},
  timestamp = {Mon, 12 Oct 2020 17:53:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-01264.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2112-09824,
  author    = {Sameer Bibikar and
               Haris Vikalo and
               Zhangyang Wang and
               Xiaohan Chen},
  title     = {Federated Dynamic Sparse Training: Computing Less, Communicating Less,
               Yet Learning Better},
  journal   = {CoRR},
  volume    = {abs/2112.09824},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.09824},
  eprinttype = {arXiv},
  eprint    = {2112.09824},
  timestamp = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-09824.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings {232971,
author = {Zheng Chai and Hannan Fayyaz and Zeshan Fayyaz and Ali Anwar and Yi Zhou and Nathalie Baracaldo and Heiko Ludwig and Yue Cheng},
title = {Towards Taming the Resource and Data Heterogeneity in Federated Learning},
booktitle = {2019 USENIX Conference on Operational Machine Learning (OpML 19)},
year = {2019},
isbn = {978-1-939133-00-7},
address = {Santa Clara, CA},
pages = {19--21},
url = {https://www.usenix.org/conference/opml19/presentation/chai},
publisher = {USENIX Association},
month = may,
}

@inproceedings{10.1145/3447993.3483278,
author = {Li, Ang and Sun, Jingwei and Li, Pengcheng and Pu, Yu and Li, Hai and Chen, Yiran},
title = {Hermes: An Efficient Federated Learning Framework for Heterogeneous Mobile Clients},
year = {2021},
isbn = {9781450383424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447993.3483278},
doi = {10.1145/3447993.3483278},
abstract = {Federated learning (FL) has been a popular method to achieve distributed machine learning among numerous devices without sharing their data to a cloud server. FL aims to learn a shared global model with the participation of massive devices under the orchestration of a central server. However, mobile devices usually have limited communication bandwidth to transfer local updates to the central server. In addition, the data residing across devices is intrinsically statistically heterogeneous (i.e., non-IID data distribution). Learning a single global model may not work well for all devices participating in the FL under data heterogeneity. Such communication cost and data heterogeneity are two critical bottlenecks that hinder from applying FL in practice. Moreover, mobile devices usually have limited computational resources. Improving the inference efficiency of the learned model is critical to deploy deep learning applications on mobile devices. In this paper, we present Hermes - a communication and inference-efficient FL framework under data heterogeneity. To this end, each device finds a small subnetwork by applying the structured pruning; only the updates of these subnetworks will be communicated between the server and the devices. Instead of taking the average over all parameters of all devices as conventional FL frameworks, the server performs the average on only overlapped parameters across each subnetwork. By applying Hermes, each device can learn a personalized and structured sparse deep neural network, which can run efficiently on devices. Experiment results show the remarkable advantages of Hermes over the status quo approaches. Hermes achieves as high as 32.17\% increase in inference accuracy, 3.48\texttimes{} reduction on the communication cost, 1.83\texttimes{} speedup in inference efficiency, and 1.8\texttimes{} savings on energy consumption.},
booktitle = {Proceedings of the 27th Annual International Conference on Mobile Computing and Networking},
pages = {420–437},
numpages = {18},
keywords = {communication efficiency, inference efficiency, personalization, federated learning, data heterogeneity},
location = {New Orleans, Louisiana},
series = {MobiCom '21}
}

@INPROCEEDINGS{9139810,
  author={Wang, Cong and Wei, Xin and Zhou, Pengzhan},
  booktitle={2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={Optimize Scheduling of Federated Learning on Battery-powered Mobile Devices}, 
  year={2020},
  volume={},
  number={},
  pages={212-221},
  doi={10.1109/IPDPS47924.2020.00031}
}

@article{DBLP:journals/corr/abs-1909-12326,
  author    = {Yuang Jiang and
               Shiqiang Wang and
               Bong Jun Ko and
               Wei{-}Han Lee and
               Leandros Tassiulas},
  title     = {Model Pruning Enables Efficient Federated Learning on Edge Devices},
  journal   = {CoRR},
  volume    = {abs/1909.12326},
  year      = {2019}
}

@article{DBLP:journals/corr/abs-1902-01046,
  author    = {Kallista A. Bonawitz and
               Hubert Eichner and
               Wolfgang Grieskamp and
               Dzmitry Huba and
               Alex Ingerman and
               Vladimir Ivanov and
               Chlo{\'{e}} Kiddon and
               Jakub Kone{\v{c}}n{\'y} and
               Stefano Mazzocchi and
               H. Brendan McMahan and
               Timon Van Overveldt and
               David Petrou and
               Daniel Ramage and
               Jason Roselander},
  title     = {Towards Federated Learning at Scale: System Design},
  journal   = {CoRR},
  volume    = {abs/1902.01046},
  year      = {2019}
}

@article{10.1504/IJMC.2005.006583, 
author = {Kamssu, Aurore J.}, 
title = {Global Connectivity through Wireless Network Technology: A Possible Solution 
for Poor Countries}, 
year = {2005}, 
issue_date = {December 2005}, 
publisher = {Inderscience Publishers}, 
address = {Geneva 15, CHE}, 
volume = {3}, 
number = {3}, 
issn = {1470-949X}, 
url = {https://doi.org/10.1504/IJMC.2005.006583}, 
doi = {10.1504/IJMC.2005.006583}, 
abstract = {There has always been an unequal access to technology among 
demographically, economically, and socially diverse groups. However, mobile 
technology seems to be defying diversity obstacles. The present study investigates the 
factors that may contribute to the prosperity of mobile technology; especially, socio-
economic factors, telecom infrastructures, and internet use. Statistical analyses show 
that there is an empirical relationship between these factors and the growth of mobile 
technology. The factors analysed reveal that, unlike other technologies, mobile 
technology has been growing faster in economically and technologically challenged 
nations; and therefore, mobile technology may be the best tool to overcome internet 
connectivity problems in poor countries.}, 
journal = {Int. J. Mob. Commun.}, 
month = {mar}, 
pages = {249–262}, 
numpages = {14}, 
keywords = {mobile communications, wireless networks, telecommunications 
infrastructures, developing countries, global internet connectivity, socio-economic 
factors, mobile technology} 
}