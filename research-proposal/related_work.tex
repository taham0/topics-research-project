\section{Related Work}
    \textbf{HeteroFL} was the first work to challenge the assumption that local models must have the same architecture as the global model.~\cite{DBLP:journals/corr/abs-2010-01264} \textbf{FedDST} proposes approaches to make on-device computation and in-network communication more efficient.~\cite{DBLP:journals/corr/abs-2112-09824} \textbf{FedProx} incorporates partial work to include low-end devices~\cite{DBLP:journals/corr/abs-1812-06127}. However, these frameworks mostly leverage the \textbf{LEAF} benchmark for experimentation.~\cite{DBLP:journals/corr/abs-1812-01097} In addition,~\cite{232971} aims to demonstrate the impact of straggler devices by measuring the impact of cpu resource heterogeneity on training time. The evaluation is conducted using an emulated environment with clients running in Docker containers deployed on a AWS EC2 Virtual Machine instance. The following FL approches perform evaluations using small-scale testbeds of real-world devices. \textbf{PruneFL} uses a set of Raspberry Pi devices connected to a central server (PC) and a simulated setting.~\cite{9762360} Time measurements from Raspberry Pi devices are used for experiments conducted on the simulated setting. \textbf{Hermes} leverages structured pruning to find a small subnetwork for each device and aggregating across overlapping parameters to learn a structured sparse deep neural network.~\cite{10.1145/3447993.3483278} The framework is evaluated on a testbed of 3 Google Pixel smartphones connected to a central server. Similarly,~\cite{9139810} utilized a testbed of 4 devices with different device characteristics to to evaluate their approach using the MNIST dataset. We find few works that have performed evaluations on real-world devices.

% reference deployed FL applications
% papers evaluated on testbed of real devices